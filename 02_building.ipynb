{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzdJq4oPZ_30"
   },
   "source": [
    "# Assignment 2: Building a Simple Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ev2AJMIZ_31"
   },
   "source": [
    "In this assignment, we will build a simple search index, which we will use later for Boolean retrieval. The assignment tasks are again at the bottom of this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edoy7Nw0Z_32"
   },
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RuJ-9QOXZ_32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Summaries_file = 'recognition_Summaries.pkl.bz2'\n",
    "Abstracts_file = 'recognition_Abstracts.pkl.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "error",
     "timestamp": 1699642890261,
     "user": {
      "displayName": "Y Wang",
      "userId": "04653720980646316291"
     },
     "user_tz": -60
    },
    "id": "Jy_tFQkeZ_32",
    "outputId": "a8e4bdb9-0464-423b-c2a1-d5ecf76547c2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle, bz2\n",
    "from collections import namedtuple\n",
    "\n",
    "Summaries = pickle.load( bz2.BZ2File( Summaries_file, 'rb' ) )\n",
    "\n",
    "paper = namedtuple( 'paper', ['title', 'authors', 'year', 'doi'] )\n",
    "\n",
    "for (id, paper_info) in Summaries.items():\n",
    "    Summaries[id] = paper( *paper_info )\n",
    "\n",
    "Abstracts = pickle.load( bz2.BZ2File( Abstracts_file, 'rb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqOk2FtCZ_33"
   },
   "source": [
    "Let's have a look at what the data looks like for an example of a paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cHKKXP1iZ_33",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper(title='Surveying the seen: 100 years of British vision.', authors=['Wade NJ', 'Bruce V'], year=2001, doi='')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summaries[11802866]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ipoZty81Z_33",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Perceptual phenomena and their interpretations have fashioned the course of psychology. This article surveys how theories of visual perception and methodologies have developed during the lifetime of the British Psychological Society. The experimental study of vision was instigated by British natural philosophers in the early nineteenth century but this impetus was not maintained thereafter. Not until the 1930s and 1940s did research on perception resume in earnest within British universities. The adoption of concepts (such as schema) potentially grounded in neural organization, particularly by Bartlett and Craik, accelerated experimental, theoretical and applied vision research. From mid-century the influence of information processing models of perception became increasingly dominant, and they were often integrated with the rapidly expanding understanding of neurophysiological underpinnings. The epitome of these developments was Marr's model of vision which, in our view, marked the start of the modern era of vision research. Computers have transformed the nature of stimulus control and response measurement in perceptual experiments. More naturalistic stimuli can be presented and manipulated, and complex behavioural responses, such as patterns of eye movements, fractionated. Non-invasive recording of brain activity to visual stimulation has similarly been transformed with a variety of methods for imaging brain activity. Neuroimaging has been applied to localizing perceptual and cognitive functions and in studying patients with known deficits in visual recognition. However, the eagerness with which the computer has been adopted by perceptual psychologists is likely to be tempered by a growing awareness of the differences between viewing scenes and simulations of them.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstracts[11802866]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMPtThheZ_34"
   },
   "source": [
    "## Some Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r88Pg3SZ_34"
   },
   "source": [
    "We'll define some utility functions that allow us to tokenize a string into terms, perform linguistic preprocessing on a list of terms, as well as a function to display information about a paper in a nice way. Note that these tokenization and preprocessing functions are rather naive. We will improve them in a later assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2MuvRQqoZ_34",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recognition', 'of', 'the', 'positive', 'effects', 'of', 'music', 'and', 'dance', 'for', 'mental', 'health.']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function that tokenizes a string in a rather naive way. Can be extended later.\n",
    "    \"\"\"\n",
    "    return text.split(' ')\n",
    "\n",
    "def preprocess(tokens):\n",
    "    \"\"\"\n",
    "    Perform linguistic preprocessing on the list of tokens. Can be extended later.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        result.append(token.lower())\n",
    "    return result\n",
    "\n",
    "print(preprocess(tokenize(\"Recognition of the positive effects of music and dance for mental health.\")))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ka9YSL4DZ_34",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3390/biomedicines10112853>Cell-Free DNA in the Pathogenesis and Therapy of Non-Infectious Inflammations and Tumors.</a></strong><br>2022. Műzes G, Bohusné Barta B, Szabó O, Horgas V, Sipos F<br><small><strong>Abstract:</strong> <em>The basic function of the immune system is the protection of the host against infections, along with the preservation of the individual antigenic identity. The process of self-tolerance covers the discrimination between self and foreign antigens, including proteins, nucleic acids, and larger molecules. Consequently, a broken immunological self-tolerance results in the development of autoimmune or autoinflammatory disorders. Immunocompetent cells express pattern-recognition receptors on their cell membrane and cytoplasm. The majority of endogenous DNA is located intracellularly within nuclei and mitochondria. However, extracellular, cell-free DNA (cfDNA) can also be detected in a variety of diseases, such as autoimmune disorders and malignancies, which has sparked interest in using cfDNA as a possible biomarker. In recent years, the widespread use of liquid biopsies and the increasing demand for screening, as well as monitoring disease activity and therapy response, have enabled the revival of cfDNA research. The majority of studies have mainly focused on the function of cfDNA as a biomarker. However, research regarding the immunological consequences of cfDNA, such as its potential immunomodulatory or therapeutic benefits, is still in its infancy. This article discusses the involvement of various DNA-sensing receptors (e.g., absent in melanoma-2; Toll-like receptor 9; cyclic GMP-AMP synthase/activator of interferon genes) in identifying host cfDNA as a potent danger-associated molecular pattern. Furthermore, we aim to summarize the results of the experimental studies that we recently performed and highlight the immunomodulatory capacity of cfDNA, and thus, the potential for possible therapeutic consideration.</em></small><br>[ID: 36359370]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "def display_summary( id, show_abstract=False, show_id=True, extra_text='' ):\n",
    "    \"\"\"\n",
    "    Function for printing a paper's summary through IPython's Rich Display System.\n",
    "    Trims long author lists, and adds a link to the paper's DOI (when available).\n",
    "    \"\"\"\n",
    "    s = Summaries[id]\n",
    "    lines = []\n",
    "    title = s.title\n",
    "    if s.doi != '':\n",
    "        title = '<a href=http://dx.doi.org/{:s}>{:s}</a>'.format(s.doi, title)\n",
    "    title = '<strong>' + title + '</strong>'\n",
    "    lines.append(title)\n",
    "    authors = ', '.join( s.authors[:20] ) + ('' if len(s.authors) <= 20 else ', ...')\n",
    "    lines.append(str(s.year) + '. ' + authors)\n",
    "    if (show_abstract):\n",
    "        lines.append('<small><strong>Abstract:</strong> <em>{:s}</em></small>'.format(Abstracts[id]))\n",
    "    if (show_id):\n",
    "        lines.append('[ID: {:d}]'.format(id))\n",
    "    if (extra_text != ''):\n",
    "         lines.append(extra_text)\n",
    "    display( HTML('<br>'.join(lines)) )\n",
    "\n",
    "display_summary(36359370, show_abstract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAgeWYIqZ_35"
   },
   "source": [
    "## Creating our first index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9rXngQsZ_35"
   },
   "source": [
    "We will now create an _inverted index_ based on the words in the titles and abstracts of the papers in our dataset. We will implement our inverted index as a Python dictionary with term strings as keys and posting lists (implemented as Python lists) as values. We include all the tokens we can find in the title and (if available) in the abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MRyxPdLWZ_35",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "# This can take a minute:\n",
    "for id in sorted(Summaries.keys()):\n",
    "    term_set = set(preprocess(tokenize(Summaries[id].title)))\n",
    "    if id in Abstracts:\n",
    "        term_set.update(preprocess(tokenize(Abstracts[id])))\n",
    "    for term in term_set:\n",
    "        inverted_index[term].append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5Mval9kZ_35"
   },
   "source": [
    "Let's see what's in the index for the example term 'guitar':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yM6FiOtFZ_35",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2343768, 17258164, 20919784, 21983182, 28479473, 32574363, 33114599, 35957448, 37707802, 38076476]\n"
     ]
    }
   ],
   "source": [
    "print(inverted_index['guitar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBbUIJXEZ_36"
   },
   "source": [
    "We can now use this inverted index to answer simple one-word queries, for example to show all papers that contain the word 'guitar':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "e7l3RXcLZ_36",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/0001-6918(90)90074-p>Components of Stroop-like interference in word reading.</a></strong><br>1990. La Heij W, Happel B, Mulder M<br><small><strong>Abstract:</strong> <em>Previous research has shown that the naming of the picture of, for example, a guitar is substantially delayed when it is accompanied by the name of an object from the same semantic category (e.g., piano) as compared to a nonword control (e.g., xxxxx). La Heij (1988a) has shown that a large part of this Stroop-like interference effect can be attributed to two semantic characteristics of the distractor word: its semantic similarity to the target picture and its semantic relevance in the task at hand. Furthermore, it was argued that the locus of these two interference effects is the process of target-name retrieval. If this is true, semantic interference effects should diminish or disappear when, instead of a picture-naming task, a word-reading task is used. In the present study this prediction is tested. The effects of four distractor characteristics are examined: semantic relatedness, semantic relevance, response set membership and wordness. In contrast to the original picture-naming task only the effect of wordness reached significance. The results of experiments 2 and 3 show that the absence of significant semantic context effects in experiment 1 is not simply due to the fact that a distractor word has less time to affect a word-reading response. The results are taken to support a name-retrieval account of semantic interference in color and picture naming.</em></small><br>[ID: 2343768]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.actpsy.2006.12.002>Evaluating the influence of the 'unity assumption' on the temporal perception of realistic audiovisual stimuli.</a></strong><br>2008. Vatakis A, Spence C<br><small><strong>Abstract:</strong> <em>Vatakis, A. and Spence, C. (in press) [Crossmodal binding: Evaluating the 'unity assumption' using audiovisual speech stimuli. Perception &Psychophysics] recently demonstrated that when two briefly presented speech signals (one auditory and the other visual) refer to the same audiovisual speech event, people find it harder to judge their temporal order than when they refer to different speech events. Vatakis and Spence argued that the 'unity assumption' facilitated crossmodal binding on the former (matching) trials by means of a process of temporal ventriloquism. In the present study, we investigated whether the 'unity assumption' would also affect the binding of non-speech stimuli (video clips of object action or musical notes). The auditory and visual stimuli were presented at a range of stimulus onset asynchronies (SOAs) using the method of constant stimuli. Participants made unspeeded temporal order judgments (TOJs) regarding which modality stream had been presented first. The auditory and visual musical and object action stimuli were either matched (e.g., the sight of a note being played on a piano together with the corresponding sound) or else mismatched (e.g., the sight of a note being played on a piano together with the sound of a guitar string being plucked). However, in contrast to the results of Vatakis and Spence's recent speech study, no significant difference in the accuracy of temporal discrimination performance for the matched versus mismatched video clips was observed. Reasons for this discrepancy are discussed.</em></small><br>[ID: 17258164]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1037/a0020934>Effects of iconicity and semantic relatedness on lexical access in american sign language.</a></strong><br>2010. Bosworth RG, Emmorey K<br><small><strong>Abstract:</strong> <em>Iconicity is a property that pervades the lexicon of many sign languages, including American Sign Language (ASL). Iconic signs exhibit a motivated, nonarbitrary mapping between the form of the sign and its meaning. We investigated whether iconicity enhances semantic priming effects for ASL and whether iconic signs are recognized more quickly than noniconic signs are (controlling for strength of iconicity, semantic relatedness, familiarity, and imageability). Twenty deaf signers made lexical decisions to the 2nd item of a prime-target pair. Iconic target signs were preceded by prime signs that were (a) iconic and semantically related, (b) noniconic and semantically related, or (c) semantically unrelated. In addition, a set of noniconic target signs was preceded by semantically unrelated primes. Significant facilitation was observed for target signs when they were preceded by semantically related primes. However, iconicity did not increase the priming effect (e.g., the target sign PIANO was primed equally by the iconic sign GUITAR and the noniconic sign MUSIC). In addition, iconic signs were not recognized faster or more accurately than were noniconic signs. These results confirm the existence of semantic priming for sign language and suggest that iconicity does not play a robust role in online lexical processing.</em></small><br>[ID: 20919784]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.neuroimage.2011.09.021>Imitation and observational learning of hand actions: prefrontal involvement and connectivity.</a></strong><br>2012. Higuchi S, Holle H, Roberts N, Eickhoff SB, Vogt S<br><small><strong>Abstract:</strong> <em>The first aim of this event-related fMRI study was to identify the neural circuits involved in imitation learning. We used a rapid imitation task where participants directly imitated pictures of guitar chords. The results provide clear evidence for the involvement of dorsolateral prefrontal cortex, as well as the fronto-parietal mirror circuit (FPMC) during action imitation when the requirements for working memory are low. Connectivity analyses further indicated a robust connectivity between left prefrontal cortex and the components of the FPMC bilaterally. We conclude that a mechanism of automatic perception-action matching alone is insufficient to account for imitation learning. Rather, the motor representation of an observed, complex action, as provided by the FPMC, only serves as the 'raw material' for higher-order supervisory and monitoring operations associated with the prefrontal cortex. The second aim of this study was to assess whether these neural circuits are also recruited during observational practice (OP, without motor execution), or only during physical practice (PP). Whereas prefrontal cortex was not consistently activated in action observation across all participants, prefrontal activation intensities did predict the behavioural practice effects, thus indicating a crucial role of prefrontal cortex also in OP. In addition, whilst OP and PP produced similar activation intensities in the FPMC when assessed during action observation, during imitative execution, the practice-related activation decreases were significantly more pronounced for PP than for OP. This dissociation indicates a lack of execution-related resources in observationally practised actions. More specifically, we found neural efficiency effects in the right motor cingulate-basal ganglia circuit and the FPMC that were only observed after PP but not after OP. Finally, we confirmed that practice generally induced activation decreases in the FPMC during both action observation and imitation sessions and outline a framework explaining the discrepant findings in the literature.</em></small><br>[ID: 21983182]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.neuroimage.2017.04.060>Using guitar learning to probe the Action Observation Network's response to visuomotor familiarity.</a></strong><br>2017. Gardner T, Aglinskas A, Cross ES<br><small><strong>Abstract:</strong> <em>Watching other people move elicits engagement of a collection of sensorimotor brain regions collectively termed the Action Observation Network (AON). An extensive literature documents more robust AON responses when observing or executing familiar compared to unfamiliar actions, as well as a positive correlation between amplitude of AON response and an observer's familiarity with an observed or executed movement. On the other hand, emerging evidence shows patterns of AON activity counter to these findings, whereby in some circumstances, unfamiliar actions lead to greater AON engagement than familiar actions. In an attempt to reconcile these conflicting findings, some have proposed that the relationship between AON response amplitude and action familiarity is nonlinear in nature. In the present study, we used an elaborate guitar training intervention to probe the relationship between movement familiarity and AON engagement during action execution and action observation tasks. Participants underwent fMRI scanning while executing one set of guitar sequences with a scanner-compatible bass guitar and observing a second set of sequences. Participants then acquired further physical practice or observational experience with half of these stimuli outside the scanner across 3 days. Participants then returned for an identical scanning session, wherein they executed and observed equal numbers of familiar (trained) and unfamiliar (untrained) guitar sequences. Via region of interest analyses, we extracted activity within AON regions engaged during both scanning sessions, and then fit linear, quadratic and cubic regression models to these data. The data best support the cubic regression models, suggesting that the response profile within key sensorimotor brain regions associated with the AON respond to action familiarity in a nonlinear manner. Moreover, by probing the subjective nature of the prediction error signal, we show results consistent with a predictive coding account of AON engagement during action observation and execution that also takes into account effects of changes in neural efficiency.</em></small><br>[ID: 28479473]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1093/jmt/thaa009>Using Contextual and Visual Cues to Improve Sung Word Recognition in Hearing Aid Users.</a></strong><br>2020. Wilhelm LA<br><small><strong>Abstract:</strong> <em>Older adults commonly experience hearing loss that negatively affects the quality of life and creates barriers to effective therapeutic interactions as well as music listening. Music therapists have the potential to address some needs of older adults, but the effectiveness of music interventions is dependent on the perception of spoken and musical stimuli. Nonauditory information, such as contextual (e.g., keywords, picture related to song) and visual cues (e.g., clear view of singer's face), can improve speech perception. The purpose of this study was to examine the benefit of contextual and visual cues on sung word recognition in the presence of guitar accompaniment. The researcher tested 24 community-dwelling older adult hearing aid (HA) users recruited through a university HA clinic and laboratory under 3 study conditions: (a) auditory stimuli only, (b) auditory stimuli with contextual cues, and (c) auditory stimuli with visual cues. Both visual and contextual nonauditory cues benefited participants on sung word recognition. Participants' music background and training were predictive of success without nonauditory cues, and visual cues provided greater benefit than contextual cues. Based on the results of this study, it is recommended that music therapists increase the accessibility of music interventions reliant upon lyric recognition through the incorporation of clear visual and contextual cues.</em></small><br>[ID: 32574363]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3390/s20216077>Guitar Chord Sensing and Recognition Using Multi-Task Learning and Physical Data Augmentation with Robotics.</a></strong><br>2020. Byambatsogt G, Choimaa L, Koutaki G<br><small><strong>Abstract:</strong> <em>In recent years, many researchers have shown increasing interest in music information retrieval (MIR) applications, with automatic chord recognition being one of the popular tasks. Many studies have achieved/demonstrated considerable improvement using deep learning based models in automatic chord recognition problems. However, most of the existing models have focused on simple chord recognition, which classifies the root note with the major, minor, and seventh chords. Furthermore, in learning-based recognition, it is critical to collect high-quality and large amounts of training data to achieve the desired performance. In this paper, we present a multi-task learning (MTL) model for a guitar chord recognition task, where the model is trained using a relatively large-vocabulary guitar chord dataset. To solve data scarcity issues, a physical data augmentation method that directly records the chord dataset from a robotic performer is employed. Deep learning based MTL is proposed to improve the performance of automatic chord recognition with the proposed physical data augmentation dataset. The proposed MTL model is compared with four baseline models and its corresponding single-task learning model using two types of datasets, including a human dataset and a human combined with the augmented dataset. The proposed methods outperform the baseline models, and the results show that most scores of the proposed multi-task learning model are better than those of the corresponding single-task learning model. The experimental results demonstrate that physical data augmentation is an effective method for increasing the dataset size for guitar chord recognition tasks.</em></small><br>[ID: 33114599]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3390/s22155892>Fuzzy Edge-Detection as a Preprocessing Layer in Deep Neural Networks for Guitar Classification.</a></strong><br>2022. Torres C, Gonzalez CI, Martinez GE<br><small><strong>Abstract:</strong> <em>Deep neural networks have demonstrated the capability of solving classification problems using hierarchical models, and fuzzy image preprocessing has proven to be efficient in handling uncertainty found in images. This paper presents the combination of fuzzy image edge-detection and the usage of a convolutional neural network for a computer vision system to classify guitar types according to their body model. The focus of this investigation is to compare the effects of performing image-preprocessing techniques on raw data (non-normalized images) with different fuzzy edge-detection methods, specifically fuzzy Sobel, fuzzy Prewitt, and fuzzy morphological gradient, before feeding the images into a convolutional neural network to perform a classification task. We propose and compare two convolutional neural network architectures to solve the task. Fuzzy edge-detection techniques are compared against their classical counterparts (Sobel, Prewitt, and morphological gradient edge-detection) and with grayscale and color images in the RGB color space. The fuzzy preprocessing methodologies highlight the most essential features of each image, achieving favorable results when compared to the classical preprocessing methodologies and against a pre-trained model with both proposed models, as well as achieving a reduction in training times of more than 20% compared to RGB images.</em></small><br>[ID: 35957448]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1167/jov.23.10.9>Viewpoint dependence and scene context effects generalize to depth rotated three-dimensional objects.</a></strong><br>2023. Kallmayer A, Võ ML, Draschkow D<br><small><strong>Abstract:</strong> <em>Viewpoint effects on object recognition interact with object-scene consistency effects. While recognition of objects seen from \"noncanonical\" viewpoints (e.g., a cup from below) is typically impeded compared to processing of objects seen from canonical viewpoints (e.g., the string-side of a guitar), this effect is reduced by meaningful scene context information. In the present study we investigated if these findings established by using photographic images, generalize to strongly noncanonical orientations of three-dimensional (3D) models of objects. Using 3D models allowed us to probe a broad range of viewpoints and empirically establish viewpoints with very strong noncanonical and canonical orientations. In Experiment 1, we presented 3D models of objects from six different viewpoints (0°, 60°, 120°, 180° 240°, 300°) in color (1a) and grayscaled (1b) in a sequential matching task. Viewpoint had a significant effect on accuracy and response times. Based on the viewpoint effect in Experiments 1a and 1b, we could empirically determine the most canonical and noncanonical viewpoints from our set of viewpoints to use in Experiment 2. In Experiment 2, participants again performed a sequential matching task, however now the objects were paired with scene backgrounds which could be either consistent (e.g., a cup in the kitchen) or inconsistent (e.g., a guitar in the bathroom) to the object. Viewpoint interacted significantly with scene consistency in that object recognition was less affected by viewpoint when consistent scene information was provided, compared to inconsistent information. Our results show that scene context supports object recognition even when using extremely noncanonical orientations of depth rotated 3D objects. This supports the important role object-scene processing plays for object constancy especially under conditions of high uncertainty.</em></small><br>[ID: 37707802]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.dib.2023.109842>A multimodal dataset for electric guitar playing technique recognition.</a></strong><br>2024. Mitsou A, Petrogianni A, Vakalaki EA, Nikou C, Psallidas T, Giannakopoulos T<br><small><strong>Abstract:</strong> <em>Automatically detecting the playing styles of musical instruments could assist in the development of intelligent software for music coaching and training. However, the respective methodologies are still at an early stage, and there are limitations in the playing techniques that can be identified. This is partly due to the limited availability of complete and real-world datasets of instrument playing styles that are mandatory to develop and train robust machine learning models. To address this issue, in this data article, we introduce a multimodal dataset consisting of 549 video samples in MP4 format, and their respective audio samples in WAV format, covering nine different electric guitar techniques in total. These samples are produced by a recruited guitar player using a smartphone device. The recording setup is designed to closely resemble real-world situations, making the dataset valuable for developing intelligent software applications that can assess the playstyle of guitar players. Furthermore, to capture the diversities that may occur in a real scenario, different exercises are performed using each technique with three different electric guitars and three different simulation amplifiers using an amplifier simulation profiler. In addition to the audio and video samples, we also provide the musescores of the exercises, making the dataset extendable to more guitar players in the future. Finally, to demonstrate the effectiveness of our dataset in developing robust machine learning models, we design a Support Vector Machine (SVM) and a Convolutional Neural Network (CNN) for classifying the guitar techniques using the audio files of the dataset. The code for the experiments is publicly available in the dataset's repository.</em></small><br>[ID: 38076476]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_word = 'guitar'\n",
    "for i in inverted_index[query_word]:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLS3Rm9iZ_36"
   },
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVhdywUWZ_36"
   },
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjEBsuRmZ_36"
   },
   "source": [
    "**Your name:** Jaspreet Singh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSVisMaSZ_36"
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Implement the function `merge_sorted_and` outlined below. This function takes two posting lists from the index that can be assumed to be sorted already (e.g. [3,5,8,11] and [5,7,8,9,11,12]), and it should return the result of the merging of the two lists with AND. The resulting list should therefore include all the elements that appear in both lists. As explained on the slides, this operation should take advantage of the input lists being sorted already, should not perform any additional sorting operation, and should go through each of the input lists just once. Then, test your function with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "l7wAWe40Z_36",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "#assuming we have two sorted posting lists as input\n",
    "\n",
    "def merge_sorted_and(sortedList1, sortedList2):\n",
    "    mergeSortedList= []\n",
    "    index1 = 0\n",
    "    index2 = 0\n",
    "    size1 = len(sortedList1)\n",
    "    size2 = len(sortedList2)\n",
    "    seen_all_index = False\n",
    "    while seen_all_index == False:\n",
    "        if((size1 != index1) and (size2 != index2) ):\n",
    "            if sortedList1[index1] == sortedList2[index2]:\n",
    "                mergeSortedList.append(sortedList1[index1])\n",
    "                index1 +=1\n",
    "                index2 +=1\n",
    "                continue\n",
    "            if(sortedList1[index1]>sortedList2[index2]):\n",
    "                index2 +=1\n",
    "                continue\n",
    "            if(sortedList2[index2]>sortedList1[index1]):\n",
    "                index1 +=1\n",
    "                continue\n",
    "        else:\n",
    "            seen_all_index = True\n",
    "    return mergeSortedList\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 8, 43, 128]\n"
     ]
    }
   ],
   "source": [
    "firstSortedList = [2,4,8,16,32, 43, 64,128,130]\n",
    "secondSortedList = [1,2,3,5,8,13,21,43,128]\n",
    "\n",
    "print(merge_sorted_and(firstSortedList,secondSortedList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3zR_ex_Z_37"
   },
   "source": [
    "### Task 2\n",
    "\n",
    "Similarly as above, implement the function `merge_sorted_or` outlined below that executes an OR merging of the lists. The resulting list should therefore include all the elements that appear in at least one of the lists. Again, this operation should take advantage of the input lists being sorted already, should not perform any additional sorting operation, and should go through each of the input lists just once. Elements that appear in both input list should only appear once in the output list. Test your function again with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "-n9GJAztZ_37",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "def merge_sorted_or(sortedList1, sortedList2):\n",
    "    mergeSortedList= []\n",
    "    index1 = 0\n",
    "    index2 = 0\n",
    "    size1 = len(sortedList1)\n",
    "    size2 = len(sortedList2)\n",
    "    seen_all_index = False\n",
    "    while seen_all_index == False:\n",
    "        if((size1 != index1) and (size2 != index2) ):\n",
    "            if sortedList1[index1] == sortedList2[index2]:\n",
    "                mergeSortedList.append(sortedList1[index1])\n",
    "                index1 +=1\n",
    "                index2 +=1\n",
    "                continue\n",
    "            if(sortedList1[index1]>sortedList2[index2]):\n",
    "                mergeSortedList.append(sortedList2[index2])\n",
    "                index2 +=1\n",
    "                continue\n",
    "            if(sortedList2[index2]>sortedList1[index1]):\n",
    "                mergeSortedList.append(sortedList1[index1])\n",
    "                index1 +=1\n",
    "                continue\n",
    "        \n",
    "        else:\n",
    "            if(size1 != index1):\n",
    "                mergeSortedList.append(sortedList1[index1])\n",
    "                index1+=1\n",
    "                continue\n",
    "            if(size2 != index2):\n",
    "                mergeSortedList.append(sortedList2[index2])\n",
    "                index2+=1\n",
    "                continue\n",
    "            else:\n",
    "                seen_all_index = True\n",
    "    return mergeSortedList\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 8, 13, 16, 21, 32, 43, 45, 49, 55, 64, 65, 128, 130, 192, 201]\n"
     ]
    }
   ],
   "source": [
    "firstSortedList = [2,4,8,16,32, 43, 64,128,130,192,201]\n",
    "secondSortedList = [1,2,3,5,8,13,21,43,45,49,55,65]\n",
    "\n",
    "\n",
    "\n",
    "print(merge_sorted_or(firstSortedList,secondSortedList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0msSbXQ1Z_37"
   },
   "source": [
    "### Task 3\n",
    "\n",
    "Construct a function called `and_query` that takes as input a single string, consisting of one or more words, and returns as function value a list of matching documents. `and_query`, as its name suggests, should require that all query terms are present in the documents of the result list.\n",
    "\n",
    "For that, access the variable `inverted_index` from above and use the method `merge_and` that you defined. Also use the `tokenize` and `preprocess` functions we defined above to tokenize and preprocess your query.\n",
    "\n",
    "Again demonstrate the working of your function with an example (choose one that leads to fewer than 100 hits to not overblow this notebook file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "dlOxYUrcZ_37",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def and_query(input_query):\n",
    "    preproccesedTokens = preprocess(tokenize(input_query))\n",
    "    \n",
    "    posting_lists = []\n",
    "\n",
    "    for token in preproccesedTokens:\n",
    "        if(token in inverted_index):\n",
    "            posting_lists.append(inverted_index[token])\n",
    "            \n",
    "    currentAndQuery = posting_lists[0]\n",
    "    for i in range(1, len(posting_lists)):\n",
    "        currentAndQuery = merge_sorted_and(currentAndQuery, posting_lists[i])\n",
    "        \n",
    "    return currentAndQuery\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32574363, 33114599, 38076476]\n"
     ]
    }
   ],
   "source": [
    "print(and_query(\"music guitar\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VRzOafAZ_37"
   },
   "source": [
    "### Task 4\n",
    "\n",
    "Construct another function called `query_or` that works in the same way as `query_and` you just implemented, but returns as function value the documents that contain _at least one_ of the words in the query, using the `merge_sorted_or` function you defined.\n",
    "\n",
    "Demonstrate the working of this function also with an example (again, choose one that leads to fewer than 100 hits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def query_or(input_query):\n",
    "    preproccesedTokens = preprocess(tokenize(input_query))\n",
    "    \n",
    "    posting_lists = []\n",
    "\n",
    "    for token in preproccesedTokens:\n",
    "        if(token in inverted_index):\n",
    "            posting_lists.append(inverted_index[token])\n",
    "            \n",
    "    currentQueryOr = posting_lists[0]\n",
    "    for i in range(1, len(posting_lists)):\n",
    "        currentQueryOr = merge_sorted_or(currentQueryOr, posting_lists[i])\n",
    "        \n",
    "    return currentQueryOr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "K1n7n-veZ_37",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65843, 1123708, 1610160, 1777964, 1807757, 2004265, 2329375, 2501961, 2637575, 2683821, 3211683, 3391940, 3835034, 3970548, 4017559, 6890395, 7002504, 7175510, 7646945, 8019865, 8019866, 8020854, 8217304, 8324639, 8466663, 8901343, 8992485, 9494655, 10415933, 10511326, 10552763, 10688276, 11079693, 11113263, 11118438, 11132035, 11299017, 11382326, 11448741, 11519812, 11824606, 11941428, 12120229, 12561204, 12632242, 12744294, 12890998, 12956512, 14574449, 14669896, 15137839, 15446713, 15553641, 15612789, 15925307, 15964491, 16293967, 16596981, 16608236, 16737285, 16881705, 16926543, 17068604, 17189596, 17438537, 17488020, 17554908, 18048008, 18055622, 18355960, 18810293, 19052540, 19067467, 19154550, 19489112, 19528028, 19609881, 19801343, 19907094, 19933774, 20047654, 20140274, 20146077, 20442968, 20477470, 20718565, 20967459, 21435473, 21548650, 21742403, 21889151, 22132537, 22255015, 22349025, 22399295, 22481163, 22563122, 22564930, 22725621, 22791258, 22918534, 23009695, 23035847, 23544352, 23686223, 23702815, 23841421, 23991189, 24000900, 24173907, 24360446, 24370597, 24405233, 24418145, 24436053, 24453117, 24461641, 24759884, 24861970, 25009519, 25095440, 25155970, 25210741, 25212169, 25303997, 25404297, 25493468, 25721103, 25756876, 25964758, 26255440, 26294754, 26304345, 26415154, 26468072, 26489214, 26497505, 26556749, 27031921, 27214157, 27240299, 27355690, 27776380, 27884717, 27928889, 28130087, 28190352, 28465679, 28490057, 28567988, 28622182, 28626484, 28792120, 28842925, 28918160, 28960220, 28983243, 29212922, 29280146, 29299926, 29606217, 29745551, 29799461, 29853055, 29978314, 30028601, 30134153, 30227155, 30361006, 30529494, 30534723, 30544660, 30688433, 30694345, 30712118, 30813417, 30832055, 30857677, 31001872, 31144579, 31147019, 31178875, 31240479, 31314734, 31438344, 31514881, 31542893, 31550400, 31730800, 31864724, 31974208, 32039810, 32057998, 32125617, 32268888, 32283830, 32360496, 32382294, 32391580, 32399588, 32580395, 32585961, 32776896, 32829161, 32911790, 32950815, 33010365, 33014634, 33020430, 33267967, 33302194, 33340794, 33350409, 33352717, 33427680, 33461704, 33521033, 33667937, 33946582, 34131991, 34136168, 34199905, 34263804, 34286731, 34381153, 34444881, 34465763, 34564323, 34710252, 34713143, 34802418, 34813407, 34945552, 34978118, 35051690, 35134727, 35182922, 35245753, 35344759, 35413778, 35458777, 35458982, 35479381, 35547192, 35592569, 35685005, 35686688, 35746261, 35755709, 35780085, 35849665, 35934378, 35934382, 36140077, 36143048, 36255986, 36287070, 36396231, 36433495, 36529787, 36731236, 36767978, 36840133, 36898336, 36981277, 37001262, 37013998, 37039647, 37112151, 37219633, 37223802, 37266138, 37297365, 37346136, 37398626, 37420591, 37447864, 37602577, 37610510, 37660090, 37676376, 37687279, 37701328, 37741265, 37908052, 37947027, 37965007, 38023044, 38084758, 38257087, 38288410, 38292343, 38307901, 38317727, 38408450, 38466671, 38499428, 38584905, 38633658, 38667992, 38766530, 38831133, 38862723, 38905284, 38931535, 38991451, 39025025, 39029627, 39057952, 39089015, 39125452, 39126692, 39131531, 39313450, 39368197, 39403618]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "print(query_or(\"apple juice\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaNZGq8PZ_38"
   },
   "source": [
    "### Task 5\n",
    "\n",
    "Why does `query_sorted_and('music guitar')` not return our example paper 28450846, even though it mentions these terms in the title and abstract? (You do not have to implement anything to fix this yet!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_vmfd1fZ_38"
   },
   "source": [
    "**Answer:** In the example paper 28450846, the word \"guitar\" is written as \"guitar,\" with a comma following it. When we tokenize this, it appears as \"guitar,\" rather than \"guitar\" alone. To handle this accurately, we need to adjust the tokenization function, which currently splits by spaces. It would be better to remove punctuation marks like \".\", \",\", \";\", and \":\" during tokenization. This issue is why the example paper isn’t appearing in our posting list results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vSIYm2rZ_38"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNrQUUY8Z_38"
   },
   "source": [
    "Submit the answers to the assignment via Canvas as a modified version of this Notebook file (file with `.ipynb` extension) that includes your code and your answers.\n",
    "\n",
    "Before submitting, restart the kernel and re-run the complete code (**Kernel > Restart & Run All**), and then check whether your assignment code still works as expected.\n",
    "\n",
    "Don't forget to add your name, and remember that the assignments have to be done **individually**, and that code sharing or copying are **strictly forbidden** and will be punished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRtM3zsmZ_38"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
